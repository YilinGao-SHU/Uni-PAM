# Uni-PAM

Multi-task perception system to simultaneously perceive various kinds of objects is essential for autonomous driving. However, existing perception frameworks for perceiving multiple kinds of objects need a multi-labeled dataset with labels of all those objects in general, which lack the flexibility to leverage task-specific datasets. In this paper, we propose Uni-PAM (Unified Perception Anything Model), with a novel training framework for multi-task perception using task prompt generation to decouple
tasks, which enables perceiving traffic signs and traffic lights in addition to lane lines and traffic elements from existing task-specific datasets without re-labeling. To the best of our knowledge, Uni-PAM is the first model can do this. By introducing the parameter-sharing decoder among tasks, we alleviate the problems of stacking task heads, including significant parameter increase, etc. Uni-PAM achieves SOTA results in multi-task algorithms without substantial increase in parameters, which also demonstrates comparable performance to existing standalone models. The efficiency of the design is validated through comprehensive ablation experiments and results.
